import whisper
import torch
import os
import time
import sys
import threading

# ğŸ§ Input audio file from argument or manual input
if len(sys.argv) > 1:
    audio_path = " ".join(sys.argv[1:]).strip('"')
else:
    audio_path = input("ğŸ¤ Enter audio file path (e.g., recording.mp3): ").strip('"')

# Show the path being checked
print(f"ğŸŸ¨ Checking path: {audio_path}")

# File validation
if not os.path.isfile(audio_path):
    print(f"âŒ File not found!\nChecked path: {os.path.abspath(audio_path)}")
    # ğŸŒŸ Whisper Usage Info
    print("\n================ Whisper Speech-to-Text ================" )
    print("Whisper by OpenAI: Automatically transcribe audio to text.")
    print("Docs: https://platform.openai.com/docs/guides/speech-to-text")
    print("\nSupported formats: mp3, wav, m4a, flac, ogg")
    print("Tips:")
    print("- Use clear, undamaged audio files.")
    print("- Path can be relative or absolute.")
    print("- Default model: small (can be changed in code)")
    print("========================================================\n")
    sys.exit(1)
else:
    print(f"âœ… Path is valid!")

allowed_ext = {'.mp3', '.wav', '.m4a', '.flac', '.ogg'}
_, ext = os.path.splitext(audio_path)
if ext.lower() not in allowed_ext:
    print(f"âŒ File format '{ext}' is not supported! Supported formats: {', '.join(allowed_ext)}")
    sys.exit(1)

# ğŸ” Device check
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ Active device: {device.upper()}")

# ğŸ“¥ Load model
model_name = "small"
print(f"ğŸ“¦ Loading model '{model_name}'...")
model = whisper.load_model(model_name, device=device)
print(f"âœ… Model '{model_name}' loaded successfully!")

# ğŸ§  Transcription process with real-time info
print("\nâ³ Transcription started. Please wait...\n")
progress = True
spinner = ['â³', 'â²ï¸', 'âŒ›', 'â°']

def show_progress():
    t = 0
    idx = 0
    while progress:
        print(f"  {spinner[idx % len(spinner)]} Processing... {t} seconds", end='\r')
        time.sleep(1)
        t += 1
        idx += 1

thread = threading.Thread(target=show_progress)
thread.start()

start = time.time()
result = model.transcribe(audio_path, task="transcribe", language="en", verbose=True)
end = time.time()
progress = False
thread.join()

print("\nâœ… Transcription finished!")
print(f"â±ï¸ Elapsed time: {end - start:.2f} seconds")